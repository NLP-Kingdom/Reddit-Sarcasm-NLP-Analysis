{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fff723636c6538b"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-03T01:55:30.570864100Z",
     "start_time": "2024-03-03T01:55:30.441387300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, mean_squared_error, make_scorer,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from xgboost import XGBClassifier  # Assuming you are using XGBoost for classification; use XGBRegressor for regression\n",
    "import xgboost as xgb \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pprint\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd0b48a29161760d"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "   label                                            comment     author  \\\n0      0                                         NC and NH.  Trumpbart   \n1      0  You do know west teams play against west teams...  Shbshb906   \n2      0  They were underdogs earlier today, but since G...   Creepeth   \n3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n4      0                    I could use one of those tools.  cush2push   \n\n            subreddit  score  ups  downs     date          created_utc  \\\n0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n\n                                      parent_comment  \n0  Yeah, I get that argument. At this point, I'd ...  \n1  The blazers and Mavericks (The wests 5 and 6 s...  \n2                            They're favored to win.  \n3                         deadass don't kill my buzz  \n4  Yep can confirm I saw the tool they use for th...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>comment</th>\n      <th>author</th>\n      <th>subreddit</th>\n      <th>score</th>\n      <th>ups</th>\n      <th>downs</th>\n      <th>date</th>\n      <th>created_utc</th>\n      <th>parent_comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NC and NH.</td>\n      <td>Trumpbart</td>\n      <td>politics</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-10</td>\n      <td>2016-10-16 23:55:23</td>\n      <td>Yeah, I get that argument. At this point, I'd ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>You do know west teams play against west teams...</td>\n      <td>Shbshb906</td>\n      <td>nba</td>\n      <td>-4</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-11</td>\n      <td>2016-11-01 00:24:10</td>\n      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>They were underdogs earlier today, but since G...</td>\n      <td>Creepeth</td>\n      <td>nfl</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2016-09</td>\n      <td>2016-09-22 21:45:37</td>\n      <td>They're favored to win.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>This meme isn't funny none of the \"new york ni...</td>\n      <td>icebrotha</td>\n      <td>BlackPeopleTwitter</td>\n      <td>-8</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-10</td>\n      <td>2016-10-18 21:03:47</td>\n      <td>deadass don't kill my buzz</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>I could use one of those tools.</td>\n      <td>cush2push</td>\n      <td>MaddenUltimateTeam</td>\n      <td>6</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-12</td>\n      <td>2016-12-30 17:00:13</td>\n      <td>Yep can confirm I saw the tool they use for th...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv('train-balanced-sarcasm.csv')\n",
    "raw_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T01:55:33.330013900Z",
     "start_time": "2024-03-03T01:55:30.499391900Z"
    }
   },
   "id": "24dae189076df892"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data-preprocessing "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c497a3e202b0d2f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drop Features\n",
    "\n",
    "Irrelevant: author, date, created_utc"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48d4d0e986e3736d"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    50016\n",
      "1    49984\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "   label                                            comment        author  \\\n0      0  The title of this article should be: \"How to n...          xNOM   \n1      0  What a wasted opportunity... at least be funny...    Jacked1218   \n2      1                              But....but... sodium!        Chicup   \n3      1      Yeah, we need more animosity between nations.  Bloodysneeze   \n4      0                                              uuugh    name032282   \n\n    subreddit  score  ups  downs     date          created_utc  \\\n0  MensRights      1    1      0  2014-11  2014-11-07 11:48:34   \n1         MMA      5    5      0  2016-07  2016-07-05 22:06:19   \n2    fatlogic      1    1      0  2015-03  2015-03-09 13:16:40   \n3   worldnews      1    1      0  2013-07  2013-07-09 15:20:36   \n4   Minecraft      2    2      0  2011-04  2011-04-02 04:50:42   \n\n                                      parent_comment  \n0        You can approach women without being creepy  \n1                          Nate Diaz Snapchat Hacked  \n2  Canned soups have been hugely helpful for the ...  \n3  It really sounds like all the English speaking...  \n4  Has anyone held a doggy funeral on their serve...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>comment</th>\n      <th>author</th>\n      <th>subreddit</th>\n      <th>score</th>\n      <th>ups</th>\n      <th>downs</th>\n      <th>date</th>\n      <th>created_utc</th>\n      <th>parent_comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>The title of this article should be: \"How to n...</td>\n      <td>xNOM</td>\n      <td>MensRights</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2014-11</td>\n      <td>2014-11-07 11:48:34</td>\n      <td>You can approach women without being creepy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>What a wasted opportunity... at least be funny...</td>\n      <td>Jacked1218</td>\n      <td>MMA</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0</td>\n      <td>2016-07</td>\n      <td>2016-07-05 22:06:19</td>\n      <td>Nate Diaz Snapchat Hacked</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>But....but... sodium!</td>\n      <td>Chicup</td>\n      <td>fatlogic</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2015-03</td>\n      <td>2015-03-09 13:16:40</td>\n      <td>Canned soups have been hugely helpful for the ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Yeah, we need more animosity between nations.</td>\n      <td>Bloodysneeze</td>\n      <td>worldnews</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2013-07</td>\n      <td>2013-07-09 15:20:36</td>\n      <td>It really sounds like all the English speaking...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>uuugh</td>\n      <td>name032282</td>\n      <td>Minecraft</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2011-04</td>\n      <td>2011-04-02 04:50:42</td>\n      <td>Has anyone held a doggy funeral on their serve...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop NAs\n",
    "raw_df.dropna(inplace=True)\n",
    "\n",
    "# Select 100000 rows of sample\n",
    "# Reset index so the cross validation later won't go wrong\n",
    "filter_df = raw_df.sample(n=100000, random_state=000).reset_index(drop=True)\n",
    "\n",
    "# Drop irrelevant features\n",
    "filter_df.drop(['author', 'date', 'created_utc'],axis=1)\n",
    "\n",
    "# Data is balance, do not need oversampling\n",
    "print(filter_df['label'].value_counts()) \n",
    "\n",
    "# Show the data\n",
    "filter_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T01:55:34.151473100Z",
     "start_time": "2024-03-03T01:55:33.331010700Z"
    }
   },
   "id": "625c11bd83cbd719"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Categorical Process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84b5a2a0de21ccb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform comments into TF-IDF vectors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c99f74fa3801eed"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 436490)\n",
      "(100000, 899057)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the processed comments and parent_comment\n",
    "tfidf_comment = tfidf_vectorizer.fit_transform(filter_df['comment'])\n",
    "tfidf_parent_comment = tfidf_vectorizer.fit_transform(filter_df['parent_comment'])\n",
    "\n",
    "# Display the shape of the resulting TF-IDF feature matrix\n",
    "print(tfidf_comment.shape)\n",
    "print(tfidf_parent_comment.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T01:55:41.312268100Z",
     "start_time": "2024-03-03T01:55:34.151473100Z"
    }
   },
   "id": "2ed96c8eed9e6054"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have got a comment TF-IDF matrix, containing 100000 rows and 436490 features\n",
    "We have got a parent_comment TF-IDF matrix, containing 100000 rows and 899057 features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "466e80682cedbfaa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform categorical \"subreddit\" to dummy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ba0c5e80b8f7858"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "(100000, 5594)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = ['subreddit']\n",
    "for i in categorical_columns: \n",
    "    filter_df = pd.concat([filter_df,pd.get_dummies(filter_df[i],drop_first=True, prefix=i)],axis=1)\n",
    "    filter_df = filter_df.drop(i,axis=1)\n",
    "    \n",
    "filter_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T01:55:42.717775200Z",
     "start_time": "2024-03-03T01:55:41.313267600Z"
    }
   },
   "id": "d61581906660faed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bb084cd41406207"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# Y is the response variable\n",
    "Y = filter_df['label']\n",
    "\n",
    "# X is the features\n",
    "X = tfidf_comment\n",
    "\n",
    "# Split the data (Train 0.8, Test 0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T01:55:42.756204200Z",
     "start_time": "2024-03-03T01:55:42.717775200Z"
    }
   },
   "id": "4de6ae5139cb55a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-Fold CV Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f80331cb0ea8defe"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# Set up K-Fold Cross Validation \n",
    "n_splits = 5\n",
    "shuffle = True\n",
    "random_state = 000\n",
    "cv = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T01:55:42.756204200Z",
     "start_time": "2024-03-03T01:55:42.738723Z"
    }
   },
   "id": "9d7c518ab970ed35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Function to Create Dictionary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c08174d5304e7ea"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def create_dictionary(param_1,param_2):\n",
    "    result_dictionary = {} \n",
    "    for i in param_1: \n",
    "        result_dictionary[i] = {} \n",
    "        for j in param_2: \n",
    "                result_dictionary[i][j] = {} \n",
    "    return result_dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T01:55:42.756204200Z",
     "start_time": "2024-03-03T01:55:42.742079Z"
    }
   },
   "id": "b6d10566970df04e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45ea6202bf1b29ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Random Forest Cross Validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3198496f12dfb10"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth of Tree :  5  Number of Trees  50\n",
      "Accuracy : 0.59803\n",
      "Precision : 0.3825504935784383\n",
      "Recall : 0.6773511191699465\n",
      "\n",
      "Depth of Tree :  10  Number of Trees  50\n",
      "Accuracy : 0.62404\n",
      "Precision : 0.467190124517038\n",
      "Recall : 0.6825602446111234\n",
      "\n",
      "Depth of Tree :  20  Number of Trees  50\n",
      "Accuracy : 0.63994\n",
      "Precision : 0.5092531345465913\n",
      "Recall : 0.6897218127610609\n",
      "\n",
      "Depth of Tree :  5  Number of Trees  100\n",
      "Accuracy : 0.62166\n",
      "Precision : 0.4726726076503482\n",
      "Recall : 0.6765865582782878\n",
      "\n",
      "Depth of Tree :  10  Number of Trees  100\n",
      "Accuracy : 0.64376\n",
      "Precision : 0.5075998038122622\n",
      "Recall : 0.7003556176931325\n",
      "\n",
      "Depth of Tree :  20  Number of Trees  100\n",
      "Accuracy : 0.6550100000000001\n",
      "Precision : 0.5329496851245575\n",
      "Recall : 0.7054703721406496\n",
      "\n",
      "Depth of Tree :  5  Number of Trees  150\n",
      "Accuracy : 0.6336600000000001\n",
      "Precision : 0.49299248489183994\n",
      "Recall : 0.6901094132379519\n",
      "\n",
      "Depth of Tree :  10  Number of Trees  150\n",
      "Accuracy : 0.65209\n",
      "Precision : 0.5186244109213808\n",
      "Recall : 0.7099828440343202\n",
      "\n",
      "Depth of Tree :  20  Number of Trees  150\n",
      "Accuracy : 0.66127\n",
      "Precision : 0.5347756155525282\n",
      "Recall : 0.716843840553465\n",
      "\n",
      "------------------\n",
      "Accuracy :  {50: {5: 0.59803, 10: 0.62404, 20: 0.63994}, 100: {5: 0.62166, 10: 0.64376, 20: 0.6550100000000001}, 150: {5: 0.6336600000000001, 10: 0.65209, 20: 0.66127}}\n",
      "Precision :  {50: {5: 0.6773511191699465, 10: 0.6825602446111234, 20: 0.6897218127610609}, 100: {5: 0.6765865582782878, 10: 0.7003556176931325, 20: 0.7054703721406496}, 150: {5: 0.6901094132379519, 10: 0.7099828440343202, 20: 0.716843840553465}}\n",
      "Recall :  {50: {5: 0.3825504935784383, 10: 0.467190124517038, 20: 0.5092531345465913}, 100: {5: 0.4726726076503482, 10: 0.5075998038122622, 20: 0.5329496851245575}, 150: {5: 0.49299248489183994, 10: 0.5186244109213808, 20: 0.5347756155525282}}\n"
     ]
    }
   ],
   "source": [
    "# CV in Trees  \n",
    "# Set Hyperparameter (Lambda) values to cross validate here !!!!! \n",
    "max_depth = [2,5,10,15,20,25]\n",
    "number_of_trees = [51,101,151,201] \n",
    "\n",
    "cross_validate_result = create_dictionary(number_of_trees,max_depth)\n",
    "cross_validate_recall = create_dictionary(number_of_trees,max_depth)\n",
    "cross_validate_precision = create_dictionary(number_of_trees,max_depth)\n",
    "cross_validate_mse = create_dictionary(number_of_trees, max_depth)\n",
    "\n",
    "with open('Random Forest CV Summary.txt', 'w') as file:\n",
    "    \n",
    "    for tree in number_of_trees: \n",
    "        for depth in max_depth:\n",
    "            print('Depth of Tree : ', depth, ' Number of Trees ', tree)\n",
    "            file.write(f'Depth of Tree : {depth}, Number of Trees: {tree}\\n')\n",
    "            \n",
    "            accuracies = []\n",
    "            recall_scores = [] \n",
    "            precision_scores = [] \n",
    "            mse_scores = []\n",
    "\n",
    "            random_forest_cv = RandomForestClassifier(n_estimators = tree, max_depth=depth)\n",
    "        \n",
    "            for train_index, test_index in cv.split(X):\n",
    "                # change to loc to define the rows in the dataframe \n",
    "                X_cv_train, X_cv_test = X[train_index], X[test_index]\n",
    "                Y_cv_train, Y_cv_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "                random_forest_cv.fit(X_cv_train, Y_cv_train)\n",
    "                Y_pred = random_forest_cv.predict(X_cv_test)\n",
    "\n",
    "                #Cross-Validation Prediction Error\n",
    "                score = random_forest_cv.score(X_cv_test, Y_cv_test)\n",
    "                accuracies.append(score)\n",
    "                recall_scores.append(recall_score(Y_cv_test, Y_pred))\n",
    "                precision_scores.append(precision_score(Y_cv_test,Y_pred))\n",
    "                mse_scores.append(mean_squared_error(Y_cv_test, Y_pred))\n",
    "        \n",
    "            cross_validate_result[tree][depth] = (sum(accuracies)/len(accuracies))\n",
    "            cross_validate_recall[tree][depth] = (sum(recall_scores)/len(recall_scores))\n",
    "            cross_validate_precision[tree][depth] = (sum(precision_scores)/len(precision_scores))\n",
    "            cross_validate_mse[tree][depth] = (sum(mse_scores) / len(mse_scores))\n",
    "            \n",
    "            file.write(f\"Accuracy: {np.mean(accuracies)}\\n\")\n",
    "            file.write(f\"Precision: {np.mean(precision_scores)}\\n\")\n",
    "            file.write(f\"Recall: {np.mean(recall_scores)}\\n\")\n",
    "            file.write(f\"MSE: {np.mean(mse_scores)}\\n\\n\")\n",
    "\n",
    "        print(\"Accuracy : \" + str((sum(accuracies)/len(accuracies))))\n",
    "        print(\"Precision : \" + str((sum(recall_scores)/len(recall_scores))))\n",
    "        print(\"Recall : \" + str((sum(precision_scores)/len(precision_scores))))\n",
    "        print(\"MSE : \" + str((sum(mse_scores) / len(mse_scores))))\n",
    "        print() \n",
    "        \n",
    "    # Dictionary Summary\n",
    "    print('------------------')\n",
    "    print('Accuracy : ', cross_validate_result)\n",
    "    print('Precision : ',cross_validate_precision)\n",
    "    print('Recall : ',cross_validate_recall)\n",
    "    print('MSE : ', cross_validate_mse)\n",
    "    \n",
    "    file.write('------------------\\n')\n",
    "    file.write('Accuracy Summary:\\n')\n",
    "    file.write(str(cross_validate_result) + '\\n')\n",
    "    file.write('Precision Summary:\\n')\n",
    "    file.write(str(cross_validate_precision) + '\\n')\n",
    "    file.write('Recall Summary:\\n')\n",
    "    file.write(str(cross_validate_recall) + '\\n')\n",
    "    file.write('MSE Summary:\\n')\n",
    "    file.write(str(cross_validate_mse) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T02:49:56.031919200Z",
     "start_time": "2024-03-03T02:06:33.403925800Z"
    }
   },
   "id": "ae676bf8916158e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91e3191274b13ebf"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[80], line 20\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m metric_name, metric_data \u001B[38;5;129;01min\u001B[39;00m metrics\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m num_trees, depths \u001B[38;5;129;01min\u001B[39;00m metric_data\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m---> 20\u001B[0m         plt\u001B[38;5;241m.\u001B[39mplot(max_depth, [depths[depth] \u001B[38;5;28;01mfor\u001B[39;00m depth \u001B[38;5;129;01min\u001B[39;00m max_depth], \n\u001B[0;32m     21\u001B[0m                  label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_trees\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m trees)\u001B[39m\u001B[38;5;124m'\u001B[39m, \n\u001B[0;32m     22\u001B[0m                  linestyle\u001B[38;5;241m=\u001B[39mline_styles[num_trees], \n\u001B[0;32m     23\u001B[0m                  color\u001B[38;5;241m=\u001B[39mcolors[metric_name])\n\u001B[0;32m     25\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModel Performance by Max Depth and Number of Trees\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     26\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMax Depth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[80], line 20\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m metric_name, metric_data \u001B[38;5;129;01min\u001B[39;00m metrics\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m num_trees, depths \u001B[38;5;129;01min\u001B[39;00m metric_data\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m---> 20\u001B[0m         plt\u001B[38;5;241m.\u001B[39mplot(max_depth, [depths[depth] \u001B[38;5;28;01mfor\u001B[39;00m depth \u001B[38;5;129;01min\u001B[39;00m max_depth], \n\u001B[0;32m     21\u001B[0m                  label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_trees\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m trees)\u001B[39m\u001B[38;5;124m'\u001B[39m, \n\u001B[0;32m     22\u001B[0m                  linestyle\u001B[38;5;241m=\u001B[39mline_styles[num_trees], \n\u001B[0;32m     23\u001B[0m                  color\u001B[38;5;241m=\u001B[39mcolors[metric_name])\n\u001B[0;32m     25\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModel Performance by Max Depth and Number of Trees\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     26\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMax Depth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: 4"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x800 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting all metrics in a single plot with different line styles and colors to distinguish them\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Define line styles and colors for different number of trees\n",
    "line_styles = {50: 'solid', 100: 'dashed', 150: 'dotted'}\n",
    "colors = {'Accuracy': 'blue', 'Precision': 'green', 'Recall': 'red', 'MSE': 'purple'}\n",
    "\n",
    "# Consolidate plotting data\n",
    "metrics = {\n",
    "    'Accuracy': cross_validate_result,\n",
    "    'Precision': cross_validate_precision,\n",
    "    'Recall': cross_validate_recall,\n",
    "    #'MSE': cross_validate_mse,\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "for metric_name, metric_data in metrics.items():\n",
    "    for num_trees, depths in metric_data.items():\n",
    "        plt.plot(max_depth, [depths[depth] for depth in max_depth], \n",
    "                 label=f'{metric_name} ({num_trees} trees)', \n",
    "                 linestyle=line_styles[num_trees], \n",
    "                 color=colors[metric_name])\n",
    "\n",
    "plt.title('Model Performance by Max Depth and Number of Trees')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T03:39:35.769228Z",
     "start_time": "2024-03-03T03:39:35.747986Z"
    }
   },
   "id": "a906cd9ccb12b32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After selected the best parameter, rerun the random forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d9e13b4a8282a84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators = 200, max_depth=5, random_state=0)\n",
    "random_forest.fit(X, Y)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "print(\"RND Forest Accuracy : \" , accuracy_score(y_test, y_pred))\n",
    "print(\"RND Forest Recall : \" , recall_score(y_test, y_pred))\n",
    "print(\"RND Forest Precision : \", precision_score(y_test,y_pred))\n",
    "print(\"RND Forest F1 : \", f1_score(y_test,y_pred))\n",
    "print(\"RND Forest MSE : \", mean_squared_error(y_test,y_pred))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae634c8cbb2dd87b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "99f04e758abaf743"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
