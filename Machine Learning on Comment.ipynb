{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fff723636c6538b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-04T22:10:15.857662700Z",
     "start_time": "2024-03-04T22:10:15.428174200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, mean_squared_error, make_scorer,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd0b48a29161760d"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "   label                                            comment     author  \\\n0      0                                         NC and NH.  Trumpbart   \n1      0  You do know west teams play against west teams...  Shbshb906   \n2      0  They were underdogs earlier today, but since G...   Creepeth   \n3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n4      0                    I could use one of those tools.  cush2push   \n\n            subreddit  score  ups  downs     date          created_utc  \\\n0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n\n                                      parent_comment  \n0  Yeah, I get that argument. At this point, I'd ...  \n1  The blazers and Mavericks (The wests 5 and 6 s...  \n2                            They're favored to win.  \n3                         deadass don't kill my buzz  \n4  Yep can confirm I saw the tool they use for th...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>comment</th>\n      <th>author</th>\n      <th>subreddit</th>\n      <th>score</th>\n      <th>ups</th>\n      <th>downs</th>\n      <th>date</th>\n      <th>created_utc</th>\n      <th>parent_comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NC and NH.</td>\n      <td>Trumpbart</td>\n      <td>politics</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-10</td>\n      <td>2016-10-16 23:55:23</td>\n      <td>Yeah, I get that argument. At this point, I'd ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>You do know west teams play against west teams...</td>\n      <td>Shbshb906</td>\n      <td>nba</td>\n      <td>-4</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-11</td>\n      <td>2016-11-01 00:24:10</td>\n      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>They were underdogs earlier today, but since G...</td>\n      <td>Creepeth</td>\n      <td>nfl</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2016-09</td>\n      <td>2016-09-22 21:45:37</td>\n      <td>They're favored to win.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>This meme isn't funny none of the \"new york ni...</td>\n      <td>icebrotha</td>\n      <td>BlackPeopleTwitter</td>\n      <td>-8</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-10</td>\n      <td>2016-10-18 21:03:47</td>\n      <td>deadass don't kill my buzz</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>I could use one of those tools.</td>\n      <td>cush2push</td>\n      <td>MaddenUltimateTeam</td>\n      <td>6</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-12</td>\n      <td>2016-12-30 17:00:13</td>\n      <td>Yep can confirm I saw the tool they use for th...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv('train-balanced-sarcasm.csv')\n",
    "raw_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T22:10:19.621095200Z",
     "start_time": "2024-03-04T22:10:16.931763800Z"
    }
   },
   "id": "24dae189076df892"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data-preprocessing "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c497a3e202b0d2f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drop Features\n",
    "\n",
    "Irrelevant: author, date, created_utc"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48d4d0e986e3736d"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    50016\n",
      "1    49984\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "   label                                            comment        author  \\\n0      0  The title of this article should be: \"How to n...          xNOM   \n1      0  What a wasted opportunity... at least be funny...    Jacked1218   \n2      1                              But....but... sodium!        Chicup   \n3      1      Yeah, we need more animosity between nations.  Bloodysneeze   \n4      0                                              uuugh    name032282   \n\n    subreddit  score  ups  downs     date          created_utc  \\\n0  MensRights      1    1      0  2014-11  2014-11-07 11:48:34   \n1         MMA      5    5      0  2016-07  2016-07-05 22:06:19   \n2    fatlogic      1    1      0  2015-03  2015-03-09 13:16:40   \n3   worldnews      1    1      0  2013-07  2013-07-09 15:20:36   \n4   Minecraft      2    2      0  2011-04  2011-04-02 04:50:42   \n\n                                      parent_comment  \n0        You can approach women without being creepy  \n1                          Nate Diaz Snapchat Hacked  \n2  Canned soups have been hugely helpful for the ...  \n3  It really sounds like all the English speaking...  \n4  Has anyone held a doggy funeral on their serve...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>comment</th>\n      <th>author</th>\n      <th>subreddit</th>\n      <th>score</th>\n      <th>ups</th>\n      <th>downs</th>\n      <th>date</th>\n      <th>created_utc</th>\n      <th>parent_comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>The title of this article should be: \"How to n...</td>\n      <td>xNOM</td>\n      <td>MensRights</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2014-11</td>\n      <td>2014-11-07 11:48:34</td>\n      <td>You can approach women without being creepy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>What a wasted opportunity... at least be funny...</td>\n      <td>Jacked1218</td>\n      <td>MMA</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0</td>\n      <td>2016-07</td>\n      <td>2016-07-05 22:06:19</td>\n      <td>Nate Diaz Snapchat Hacked</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>But....but... sodium!</td>\n      <td>Chicup</td>\n      <td>fatlogic</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2015-03</td>\n      <td>2015-03-09 13:16:40</td>\n      <td>Canned soups have been hugely helpful for the ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Yeah, we need more animosity between nations.</td>\n      <td>Bloodysneeze</td>\n      <td>worldnews</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2013-07</td>\n      <td>2013-07-09 15:20:36</td>\n      <td>It really sounds like all the English speaking...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>uuugh</td>\n      <td>name032282</td>\n      <td>Minecraft</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2011-04</td>\n      <td>2011-04-02 04:50:42</td>\n      <td>Has anyone held a doggy funeral on their serve...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop NAs\n",
    "raw_df.dropna(inplace=True)\n",
    "\n",
    "# Select 100000 rows of sample\n",
    "# Reset index so the cross validation later won't go wrong\n",
    "filter_df = raw_df.sample(n=100000, random_state=000).reset_index(drop=True)\n",
    "\n",
    "# Drop irrelevant features\n",
    "filter_df.drop(['author', 'date', 'created_utc'],axis=1)\n",
    "\n",
    "# Data is balance, do not need oversampling\n",
    "print(filter_df['label'].value_counts()) \n",
    "\n",
    "# Show the data\n",
    "filter_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:18:39.829461600Z",
     "start_time": "2024-03-04T14:18:39.186221800Z"
    }
   },
   "id": "625c11bd83cbd719"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Categorical Process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84b5a2a0de21ccb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform comments into TF-IDF vectors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c99f74fa3801eed"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 436490)\n",
      "(100000, 899057)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the processed comments and parent_comment\n",
    "tfidf_comment = tfidf_vectorizer.fit_transform(filter_df['comment'])\n",
    "tfidf_parent_comment = tfidf_vectorizer.fit_transform(filter_df['parent_comment'])\n",
    "\n",
    "# Display the shape of the resulting TF-IDF feature matrix\n",
    "print(tfidf_comment.shape)\n",
    "print(tfidf_parent_comment.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:18:49.297619700Z",
     "start_time": "2024-03-04T14:18:41.548644400Z"
    }
   },
   "id": "2ed96c8eed9e6054"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have got a comment TF-IDF matrix, containing 100000 rows and 436490 features\n",
    "We have got a parent_comment TF-IDF matrix, containing 100000 rows and 899057 features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "466e80682cedbfaa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform categorical \"subreddit\" to dummy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ba0c5e80b8f7858"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(100000, 5594)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = ['subreddit']\n",
    "for i in categorical_columns: \n",
    "    filter_df = pd.concat([filter_df,pd.get_dummies(filter_df[i],drop_first=True, prefix=i)],axis=1)\n",
    "    filter_df = filter_df.drop(i,axis=1)\n",
    "    \n",
    "filter_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:18:51.213573Z",
     "start_time": "2024-03-04T14:18:49.298619200Z"
    }
   },
   "id": "d61581906660faed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bb084cd41406207"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Y is the response variable\n",
    "Y = filter_df['label']\n",
    "\n",
    "# X is the features\n",
    "X = tfidf_comment\n",
    "\n",
    "# Split the data (Train 0.8, Test 0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:18:51.230502700Z",
     "start_time": "2024-03-04T14:18:51.207703100Z"
    }
   },
   "id": "4de6ae5139cb55a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-Fold CV Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f80331cb0ea8defe"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Set up K-Fold Cross Validation \n",
    "n_splits = 5\n",
    "shuffle = True\n",
    "random_state = 000\n",
    "cv = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:18:51.233542800Z",
     "start_time": "2024-03-04T14:18:51.229394900Z"
    }
   },
   "id": "9d7c518ab970ed35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Function to Create Dictionary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c08174d5304e7ea"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def create_dictionary(param_1,param_2):\n",
    "    result_dictionary = {} \n",
    "    for i in param_1: \n",
    "        result_dictionary[i] = {} \n",
    "        for j in param_2: \n",
    "                result_dictionary[i][j] = {} \n",
    "    return result_dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:18:51.273567600Z",
     "start_time": "2024-03-04T14:18:51.233542800Z"
    }
   },
   "id": "b6d10566970df04e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45ea6202bf1b29ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Random Forest Cross Validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3198496f12dfb10"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth of Tree :  2  Number of Trees  50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 27\u001B[0m\n\u001B[0;32m     24\u001B[0m X_cv_train, X_cv_test \u001B[38;5;241m=\u001B[39m X[train_index], X[test_index]\n\u001B[0;32m     25\u001B[0m Y_cv_train, Y_cv_test \u001B[38;5;241m=\u001B[39m Y[train_index], Y[test_index]\n\u001B[1;32m---> 27\u001B[0m random_forest_cv\u001B[38;5;241m.\u001B[39mfit(X_cv_train, Y_cv_train)\n\u001B[0;32m     28\u001B[0m Y_pred \u001B[38;5;241m=\u001B[39m random_forest_cv\u001B[38;5;241m.\u001B[39mpredict(X_cv_test)\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# Cross-Validation Prediction Error\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    462\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    463\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m    464\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[0;32m    465\u001B[0m ]\n\u001B[0;32m    467\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[0;32m    468\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[0;32m    469\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[0;32m    470\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[0;32m    471\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[0;32m    472\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[1;32m--> 473\u001B[0m trees \u001B[38;5;241m=\u001B[39m Parallel(\n\u001B[0;32m    474\u001B[0m     n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs,\n\u001B[0;32m    475\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[0;32m    476\u001B[0m     prefer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthreads\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    477\u001B[0m )(\n\u001B[0;32m    478\u001B[0m     delayed(_parallel_build_trees)(\n\u001B[0;32m    479\u001B[0m         t,\n\u001B[0;32m    480\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbootstrap,\n\u001B[0;32m    481\u001B[0m         X,\n\u001B[0;32m    482\u001B[0m         y,\n\u001B[0;32m    483\u001B[0m         sample_weight,\n\u001B[0;32m    484\u001B[0m         i,\n\u001B[0;32m    485\u001B[0m         \u001B[38;5;28mlen\u001B[39m(trees),\n\u001B[0;32m    486\u001B[0m         verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[0;32m    487\u001B[0m         class_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_weight,\n\u001B[0;32m    488\u001B[0m         n_samples_bootstrap\u001B[38;5;241m=\u001B[39mn_samples_bootstrap,\n\u001B[0;32m    489\u001B[0m     )\n\u001B[0;32m    490\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(trees)\n\u001B[0;32m    491\u001B[0m )\n\u001B[0;32m    493\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[0;32m    494\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     58\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     59\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     60\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     62\u001B[0m )\n\u001B[1;32m---> 63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1085\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[0;32m   1086\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1088\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[0;32m   1089\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1092\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[0;32m   1093\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[0;32m   1094\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    899\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    900\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 901\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dispatch(tasks)\n\u001B[0;32m    902\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    817\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    818\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 819\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mapply_async(batch, callback\u001B[38;5;241m=\u001B[39mcb)\n\u001B[0;32m    820\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    821\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    822\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    823\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    824\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m ImmediateResult(func)\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    594\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    595\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    596\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 597\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m batch()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    121\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001B[0m\n\u001B[0;32m    181\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m class_weight \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced_subsample\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    182\u001B[0m         curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m, y, indices\u001B[38;5;241m=\u001B[39mindices)\n\u001B[1;32m--> 184\u001B[0m     tree\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39mcurr_sample_weight, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    186\u001B[0m     tree\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39msample_weight, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001B[0m, in \u001B[0;36mDecisionTreeClassifier.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m    859\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    860\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001B[39;00m\n\u001B[0;32m    861\u001B[0m \n\u001B[0;32m    862\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    886\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m    887\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 889\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mfit(\n\u001B[0;32m    890\u001B[0m         X,\n\u001B[0;32m    891\u001B[0m         y,\n\u001B[0;32m    892\u001B[0m         sample_weight\u001B[38;5;241m=\u001B[39msample_weight,\n\u001B[0;32m    893\u001B[0m         check_input\u001B[38;5;241m=\u001B[39mcheck_input,\n\u001B[0;32m    894\u001B[0m     )\n\u001B[0;32m    895\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001B[0m, in \u001B[0;36mBaseDecisionTree.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m    368\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    369\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[0;32m    370\u001B[0m         splitter,\n\u001B[0;32m    371\u001B[0m         min_samples_split,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    376\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[0;32m    377\u001B[0m     )\n\u001B[1;32m--> 379\u001B[0m builder\u001B[38;5;241m.\u001B[39mbuild(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtree_, X, y, sample_weight)\n\u001B[0;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    382\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# CV in Trees\n",
    "# Set Hyperparameter (Lambda) values to cross validate\n",
    "max_depth = [2, 5, 10, 15, 20, 25]\n",
    "number_of_trees = [50, 100, 150, 200]\n",
    "\n",
    "cross_validate_result = create_dictionary(number_of_trees, max_depth)\n",
    "cross_validate_recall = create_dictionary(number_of_trees, max_depth)\n",
    "cross_validate_precision = create_dictionary(number_of_trees, max_depth)\n",
    "cross_validate_mse = create_dictionary(number_of_trees, max_depth)\n",
    "\n",
    "for tree in number_of_trees:\n",
    "    for depth in max_depth:\n",
    "        print('Depth of Tree : ', depth, ' Number of Trees ', tree)\n",
    "\n",
    "        accuracies = []\n",
    "        recall_scores = []\n",
    "        precision_scores = []\n",
    "        mse_scores = []\n",
    "\n",
    "        random_forest_cv = RandomForestClassifier(n_estimators=tree, max_depth=depth)\n",
    "\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            # define the rows in the dataframe\n",
    "            X_cv_train, X_cv_test = X[train_index], X[test_index]\n",
    "            Y_cv_train, Y_cv_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            random_forest_cv.fit(X_cv_train, Y_cv_train)\n",
    "            Y_pred = random_forest_cv.predict(X_cv_test)\n",
    "\n",
    "            # Cross-Validation Prediction Error\n",
    "            score = random_forest_cv.score(X_cv_test, Y_cv_test)\n",
    "            accuracies.append(score)\n",
    "            recall_scores.append(recall_score(Y_cv_test, Y_pred))\n",
    "            precision_scores.append(precision_score(Y_cv_test, Y_pred))\n",
    "            mse_scores.append(mean_squared_error(Y_cv_test, Y_pred))\n",
    "\n",
    "        cross_validate_result[tree][depth] = (sum(accuracies) / len(accuracies))\n",
    "        cross_validate_recall[tree][depth] = (sum(recall_scores) / len(recall_scores))\n",
    "        cross_validate_precision[tree][depth] = (sum(precision_scores) / len(precision_scores))\n",
    "        cross_validate_mse[tree][depth] = (sum(mse_scores) / len(mse_scores))\n",
    "\n",
    "        print(\"Accuracy : \" + str((sum(accuracies) / len(accuracies))))\n",
    "        print(\"Precision : \" + str((sum(recall_scores) / len(recall_scores))))\n",
    "        print(\"Recall : \" + str((sum(precision_scores) / len(precision_scores))))\n",
    "        print(\"MSE : \" + str((sum(mse_scores) / len(mse_scores))))\n",
    "        print()\n",
    "\n",
    "# Dictionary Summary\n",
    "print('------------------')\n",
    "print('Accuracy : ', cross_validate_result)\n",
    "print('Precision : ', cross_validate_precision)\n",
    "print('Recall : ', cross_validate_recall)\n",
    "print('MSE : ', cross_validate_mse)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:19:06.075708200Z",
     "start_time": "2024-03-04T14:18:51.243926800Z"
    }
   },
   "id": "ae676bf8916158e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91e3191274b13ebf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Define line styles and colors for different number of trees\n",
    "line_styles = {50: 'solid', 100: 'dashed', 150: 'dotted', 200: 'dashdot'}\n",
    "colors = {'Accuracy': 'blue', 'Precision': 'green', 'Recall': 'red', 'MSE': 'grey'}\n",
    "\n",
    "# Consolidate plotting data\n",
    "metrics = {\n",
    "    'Accuracy': cross_validate_result,\n",
    "    'Precision': cross_validate_precision,\n",
    "    'Recall': cross_validate_recall,\n",
    "    'MSE': cross_validate_mse,\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "for metric_name, metric_data in metrics.items():\n",
    "    for num_trees, depths in metric_data.items():\n",
    "        plt.plot(max_depth, [depths[depth] for depth in max_depth],\n",
    "                 label=f'{metric_name} ({num_trees} trees)',\n",
    "                 linestyle=line_styles[num_trees],\n",
    "                 color=colors[metric_name])\n",
    "\n",
    "plt.title('Model Performance by Max Depth and Number of Trees')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:19:06.076694Z",
     "start_time": "2024-03-04T14:19:06.076694Z"
    }
   },
   "id": "a906cd9ccb12b32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After selected the best parameter, rerun the random forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d9e13b4a8282a84"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RND Forest Accuracy :  0.67955\n",
      "RND Forest Recall :  0.54134739775102\n",
      "RND Forest Precision :  0.7513812154696132\n",
      "RND Forest F1 :  0.6293018682399214\n",
      "RND Forest MSE :  0.32045\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators = 200, max_depth=10, random_state=0)\n",
    "random_forest.fit(X, Y)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "print(\"RND Forest Accuracy : \" , accuracy_score(y_test, y_pred))\n",
    "print(\"RND Forest Recall : \" , recall_score(y_test, y_pred))\n",
    "print(\"RND Forest Precision : \", precision_score(y_test,y_pred))\n",
    "print(\"RND Forest F1 : \", f1_score(y_test,y_pred))\n",
    "print(\"RND Forest MSE : \", mean_squared_error(y_test,y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:19:28.409531Z",
     "start_time": "2024-03-04T14:19:14.340878100Z"
    }
   },
   "id": "ae634c8cbb2dd87b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract the random forest coefficient to see its importance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8c2576652c2a147"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korea as: 0.018067620033052803\n",
      "ever fuck: 0.013952278437304124\n",
      "its resultant: 0.013840174074728347\n",
      "an anw: 0.011683601548443914\n",
      "gemini packs: 0.010402768643062815\n",
      "60fps causes: 0.00915259126627523\n",
      "industries: 0.00870759920733005\n",
      "after grabbing: 0.008363286304726193\n",
      "any children: 0.00819573069360325\n",
      "bnought: 0.007905753745770731\n",
      "korean actresses: 0.007071849573782988\n",
      "benefit packages: 0.006161003738678179\n",
      "landslide what: 0.006098612167178326\n",
      "from carragher: 0.005876114716183946\n",
      "healer the: 0.005719511660068971\n",
      "grandkid in: 0.005637473798108182\n",
      "judge situations: 0.005445803195259783\n",
      "lakeland: 0.005427689884951107\n",
      "education invented: 0.005414044153339989\n",
      "88k: 0.005401172948322826\n",
      "itt religious: 0.005234148684748423\n",
      "education finally: 0.005175563723156406\n",
      "kkk initially: 0.005164909455506009\n",
      "effect fans: 0.005116097524785196\n",
      "hike mt: 0.005015507141179427\n",
      "an mlb: 0.004889067697284597\n",
      "damned day: 0.004809951815977462\n",
      "fail compilation: 0.0045756548450866154\n",
      "japan ness: 0.004410756894164095\n",
      "khalsa: 0.00424256912683314\n",
      "establishments there: 0.004037413166168638\n",
      "it just: 0.004000545083574339\n",
      "aim though: 0.003863166476887854\n",
      "barcelona as: 0.003846247742836527\n",
      "ensure the: 0.003742939840902786\n",
      "amazingly: 0.0037131683606563158\n",
      "70pts on: 0.0036529438335101212\n",
      "awful person: 0.003631741457317129\n",
      "are tricked: 0.003567006435404559\n",
      "if he: 0.0035508241118378837\n",
      "front realized: 0.0034584942508955213\n",
      "is city: 0.003425875394138583\n",
      "korean female: 0.0033754983690966723\n",
      "ie leafs: 0.0033522416947706734\n",
      "behaviour by: 0.0033515845451091157\n",
      "he demonstrated: 0.0033044073528149787\n",
      "class athlete: 0.0032195568017906063\n",
      "is leaking: 0.0032183910378324964\n",
      "gen: 0.0031942091389174643\n",
      "his stamina: 0.0031775809793217007\n",
      "as thumbs: 0.003142262663749498\n",
      "facebook the: 0.003118524285933092\n",
      "landing my: 0.003062587384492588\n",
      "be warned: 0.003000506272516089\n",
      "any emotions: 0.002991796794481755\n",
      "complainers about: 0.002981310272777901\n",
      "fanfiction if: 0.002892008815907434\n",
      "ifs ands: 0.0028674397585075396\n",
      "guy claimed: 0.0028470922165165623\n",
      "itself leaving: 0.0028401851199033198\n",
      "from buckyballs: 0.0028385695237267577\n",
      "fingers clearly: 0.0028347272396111213\n",
      "constantly looking: 0.0027636504386183247\n",
      "ignore gay: 0.00267231246089849\n",
      "and kino: 0.0026474684718665515\n",
      "cavaliers washington: 0.0025911309533140814\n",
      "his purse: 0.0025900688968328784\n",
      "labor in: 0.00257648254882799\n",
      "dinner my: 0.0025099669967102683\n",
      "centuries also: 0.0024299510526272234\n",
      "by contraception: 0.0024139659090060624\n",
      "had great: 0.002385611828476297\n",
      "discover: 0.0023247326449714247\n",
      "fooled but: 0.002242192507615416\n",
      "6970: 0.002235127214149565\n",
      "anyhow that: 0.0021847960990432434\n",
      "epic would: 0.0021582563591787305\n",
      "but sweating: 0.00212946843252122\n",
      "console demos: 0.0021078211481853084\n",
      "electronic music: 0.00209979170662796\n",
      "headshot city: 0.002087383999842505\n",
      "credit farm: 0.0020731226556220837\n",
      "brain says: 0.0020715431331006368\n",
      "an officially: 0.0020696813927053984\n",
      "an mma: 0.002060218677247783\n",
      "emirati policeman: 0.0020601097579027237\n",
      "been nationalized: 0.0020466250090584877\n",
      "any parts: 0.0020457198265445174\n",
      "approval voting: 0.0020346799056303834\n",
      "last kick: 0.0020266322785274125\n",
      "charges or: 0.002020679852591117\n",
      "health lack: 0.002019952799445587\n",
      "appropriate rename: 0.0019685822931991585\n",
      "as singles: 0.0019428607711086834\n",
      "large ish: 0.001935382188641302\n",
      "communication above: 0.0019074841683555793\n",
      "experience then: 0.0018857141150100875\n",
      "extension cord: 0.0018622913195264705\n",
      "beneficial the: 0.001839637740996287\n",
      "huge part: 0.0018203897188059422\n"
     ]
    }
   ],
   "source": [
    "# Get Feature Importance\n",
    "importance = random_forest.feature_importances_\n",
    "\n",
    "# Get Feature Name\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Pair Feature Importance and Name\n",
    "features_importance = zip(feature_names, importance)\n",
    "\n",
    "# Sort feature by importance\n",
    "sorted_features = sorted(features_importance, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get first 50\n",
    "for feature, importance in sorted_features[:100]:\n",
    "    print(f\"{feature}: {importance}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:20:08.892330Z",
     "start_time": "2024-03-04T14:20:07.559974100Z"
    }
   },
   "id": "f06ac67763bc9d87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eliminate Stop Words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b18006bc4b4b68a5"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korea as: 0.018067620033052803\n",
      "ever fuck: 0.013952278437304124\n",
      "its resultant: 0.013840174074728347\n",
      "an anw: 0.011683601548443914\n",
      "gemini packs: 0.010402768643062815\n",
      "60fps causes: 0.00915259126627523\n",
      "industries: 0.00870759920733005\n",
      "after grabbing: 0.008363286304726193\n",
      "any children: 0.00819573069360325\n",
      "bnought: 0.007905753745770731\n",
      "korean actresses: 0.007071849573782988\n",
      "benefit packages: 0.006161003738678179\n",
      "landslide what: 0.006098612167178326\n",
      "from carragher: 0.005876114716183946\n",
      "healer the: 0.005719511660068971\n",
      "grandkid in: 0.005637473798108182\n",
      "judge situations: 0.005445803195259783\n",
      "lakeland: 0.005427689884951107\n",
      "education invented: 0.005414044153339989\n",
      "88k: 0.005401172948322826\n",
      "itt religious: 0.005234148684748423\n",
      "education finally: 0.005175563723156406\n",
      "kkk initially: 0.005164909455506009\n",
      "effect fans: 0.005116097524785196\n",
      "hike mt: 0.005015507141179427\n",
      "an mlb: 0.004889067697284597\n",
      "damned day: 0.004809951815977462\n",
      "fail compilation: 0.0045756548450866154\n",
      "japan ness: 0.004410756894164095\n",
      "khalsa: 0.00424256912683314\n",
      "establishments there: 0.004037413166168638\n",
      "it just: 0.004000545083574339\n",
      "aim though: 0.003863166476887854\n",
      "barcelona as: 0.003846247742836527\n",
      "ensure the: 0.003742939840902786\n",
      "amazingly: 0.0037131683606563158\n",
      "70pts on: 0.0036529438335101212\n",
      "awful person: 0.003631741457317129\n",
      "are tricked: 0.003567006435404559\n",
      "if he: 0.0035508241118378837\n",
      "front realized: 0.0034584942508955213\n",
      "is city: 0.003425875394138583\n",
      "korean female: 0.0033754983690966723\n",
      "ie leafs: 0.0033522416947706734\n",
      "behaviour by: 0.0033515845451091157\n",
      "he demonstrated: 0.0033044073528149787\n",
      "class athlete: 0.0032195568017906063\n",
      "is leaking: 0.0032183910378324964\n",
      "gen: 0.0031942091389174643\n",
      "his stamina: 0.0031775809793217007\n",
      "as thumbs: 0.003142262663749498\n",
      "facebook the: 0.003118524285933092\n",
      "landing my: 0.003062587384492588\n",
      "be warned: 0.003000506272516089\n",
      "any emotions: 0.002991796794481755\n",
      "complainers about: 0.002981310272777901\n",
      "fanfiction if: 0.002892008815907434\n",
      "ifs ands: 0.0028674397585075396\n",
      "guy claimed: 0.0028470922165165623\n",
      "itself leaving: 0.0028401851199033198\n",
      "from buckyballs: 0.0028385695237267577\n",
      "fingers clearly: 0.0028347272396111213\n",
      "constantly looking: 0.0027636504386183247\n",
      "ignore gay: 0.00267231246089849\n",
      "and kino: 0.0026474684718665515\n",
      "cavaliers washington: 0.0025911309533140814\n",
      "his purse: 0.0025900688968328784\n",
      "labor in: 0.00257648254882799\n",
      "dinner my: 0.0025099669967102683\n",
      "centuries also: 0.0024299510526272234\n",
      "by contraception: 0.0024139659090060624\n",
      "had great: 0.002385611828476297\n",
      "discover: 0.0023247326449714247\n",
      "fooled but: 0.002242192507615416\n",
      "6970: 0.002235127214149565\n",
      "anyhow that: 0.0021847960990432434\n",
      "epic would: 0.0021582563591787305\n",
      "but sweating: 0.00212946843252122\n",
      "console demos: 0.0021078211481853084\n",
      "electronic music: 0.00209979170662796\n",
      "headshot city: 0.002087383999842505\n",
      "credit farm: 0.0020731226556220837\n",
      "brain says: 0.0020715431331006368\n",
      "an officially: 0.0020696813927053984\n",
      "an mma: 0.002060218677247783\n",
      "emirati policeman: 0.0020601097579027237\n",
      "been nationalized: 0.0020466250090584877\n",
      "any parts: 0.0020457198265445174\n",
      "approval voting: 0.0020346799056303834\n",
      "last kick: 0.0020266322785274125\n",
      "charges or: 0.002020679852591117\n",
      "health lack: 0.002019952799445587\n",
      "appropriate rename: 0.0019685822931991585\n",
      "as singles: 0.0019428607711086834\n",
      "large ish: 0.001935382188641302\n",
      "communication above: 0.0019074841683555793\n",
      "experience then: 0.0018857141150100875\n",
      "extension cord: 0.0018622913195264705\n",
      "beneficial the: 0.001839637740996287\n",
      "huge part: 0.0018203897188059422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\16920\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter Stop Words\n",
    "filtered_features = [(feature, importance) for feature, importance in sorted_features if feature not in stop_words]\n",
    "\n",
    "# Print Filtered Features\n",
    "for feature, importance in filtered_features[:100]: # Get First 100\n",
    "    print(f\"{feature}: {importance}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:20:11.719803900Z",
     "start_time": "2024-03-04T14:20:11.625730400Z"
    }
   },
   "id": "c9d30be5beb4129"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bd0609a3e7a8ea"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "features = [feature for feature, _ in filtered_features]\n",
    "importances = [importance for _, importance in filtered_features]\n",
    "\n",
    "# \n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(features, importances, color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()  # y\n",
    "plt.show()\n",
    "\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:26:31.040799500Z",
     "start_time": "2024-03-04T14:20:51.225256200Z"
    }
   },
   "id": "c1e65e201e6236a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=importances, y=features, palette=\"viridis\")\n",
    "\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Top Features Importance from Random Forest (Filtered)')\n",
    "plt.show()\n",
    "\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-04T14:26:33.391365500Z"
    }
   },
   "id": "ef27a8ab19ec90bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adaboost"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d90305e2a931fd29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adaptive Boosting Algorithm Cross Validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9e493ab854ab044"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth of Tree :  50  Learning rate  0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mse_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 36\u001B[0m\n\u001B[0;32m     34\u001B[0m     recall_scores\u001B[38;5;241m.\u001B[39mappend(recall_score(Y_cv_test, Y_pred))\n\u001B[0;32m     35\u001B[0m     precision_scores\u001B[38;5;241m.\u001B[39mappend(precision_score(Y_cv_test,Y_pred))\n\u001B[1;32m---> 36\u001B[0m     mse_scores\u001B[38;5;241m.\u001B[39mappend(mse_score(Y_cv_test,Y_pred))\n\u001B[0;32m     38\u001B[0m cross_validate_result[rate][tree] \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28msum\u001B[39m(accuracies)\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(accuracies))\n\u001B[0;32m     39\u001B[0m cross_validate_recall[rate][tree] \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28msum\u001B[39m(recall_scores)\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(recall_scores))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'mse_score' is not defined"
     ]
    }
   ],
   "source": [
    "# CV in Trees \n",
    "# Set Hyperparameter (Lambda) values to cross validate\n",
    "\n",
    "# Create a weak learner (a stump)\n",
    "weak_learner = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "learning_rates = [0, 0.001, 0.01, 0.1, 0.5, 1]\n",
    "number_of_trees_ada = [50, 100, 150, 200]\n",
    "\n",
    "cross_validate_result = create_dictionary(learning_rates,number_of_trees_ada)\n",
    "cross_validate_recall = create_dictionary(learning_rates,number_of_trees_ada)\n",
    "cross_validate_precision = create_dictionary(learning_rates,number_of_trees_ada) \n",
    "cross_validate_mse = create_dictionary(learning_rates,number_of_trees_ada)\n",
    "\n",
    "for rate in learning_rates: \n",
    "    for tree in number_of_trees_ada:\n",
    "        print('Depth of Tree : ', tree, ' Learning rate ', rate)\n",
    "        accuracies = []\n",
    "        recall_scores = [] \n",
    "        precision_scores = [] \n",
    "        mse_scores = []\n",
    "        adaboost_classifier = AdaBoostClassifier(estimator=weak_learner, n_estimators=tree, random_state=0)\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            # define the rows in the dataframe\n",
    "            X_cv_train, X_cv_test = X[train_index], X[test_index]\n",
    "            Y_cv_train, Y_cv_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            adaboost_classifier.fit(X_cv_train, Y_cv_train)\n",
    "            Y_pred = adaboost_classifier.predict(X_cv_test)\n",
    "\n",
    "            #Cross-Validation Prediction Error\n",
    "            score = adaboost_classifier.score(X_cv_test, Y_cv_test)\n",
    "            accuracies.append(score)\n",
    "            recall_scores.append(recall_score(Y_cv_test, Y_pred))\n",
    "            precision_scores.append(precision_score(Y_cv_test,Y_pred))\n",
    "            mse_scores.append(mean_squared_error(Y_cv_test, Y_pred))\n",
    "        \n",
    "        cross_validate_result[rate][tree] = (sum(accuracies)/len(accuracies))\n",
    "        cross_validate_recall[rate][tree] = (sum(recall_scores)/len(recall_scores))\n",
    "        cross_validate_precision[rate][tree] = (sum(precision_scores)/len(precision_scores))\n",
    "        cross_validate_mse[rate][tree] = (sum(mse_scores)/len(mse_scores))\n",
    "\n",
    "        print(\"Accuracy : \" + str((sum(accuracies)/len(accuracies))))\n",
    "        print(\"Precision : \" + str((sum(recall_scores)/len(recall_scores))))\n",
    "        print(\"Recall : \" + str((sum(precision_scores)/len(precision_scores))))\n",
    "        print(\"MSE : \" + str((sum(mse_scores)/len(mse_scores))))\n",
    "        print() \n",
    "\n",
    "# Dictionary Summary\n",
    "print('------------------')\n",
    "print('Accuracy : ', cross_validate_result)\n",
    "print('Precision : ', cross_validate_precision)\n",
    "print('Recall : ', cross_validate_recall)\n",
    "print('MSE : ', cross_validate_mse)\n",
    "\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T11:53:44.069049500Z",
     "start_time": "2024-03-04T11:22:22.844599600Z"
    }
   },
   "id": "fe2424d1dc8a2f5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc6f30fe6c93ef0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Define line styles and colors for different number of trees\n",
    "line_styles = {50: 'solid', 100: 'dashed', 150: 'dotted', 200: 'dashdot'}\n",
    "colors = {'Accuracy': 'blue', 'Precision': 'green', 'Recall': 'red', 'MSE': 'grey'}\n",
    "\n",
    "# Consolidate plotting data\n",
    "metrics = {\n",
    "    'Accuracy': cross_validate_result,\n",
    "    'Precision': cross_validate_precision,\n",
    "    'Recall': cross_validate_recall,\n",
    "    'MSE': cross_validate_mse,\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "for metric_name, metric_data in metrics.items():\n",
    "    for num_trees, rates in metric_data.items():\n",
    "        plt.plot(number_of_trees_ada, [rates[rate] for rate in rates],\n",
    "                 label=f'{metric_name} ({num_trees} trees)',\n",
    "                 linestyle=line_styles[num_trees],\n",
    "                 color=colors[metric_name])\n",
    "\n",
    "plt.title('Model Performance by Learning Rate and Number of Trees')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "629267a2b7400ca8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
