{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fff723636c6538b"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-05T20:19:38.405535500Z",
     "start_time": "2024-03-05T20:19:38.377868200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, mean_squared_error, make_scorer,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd0b48a29161760d"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "   label                                            comment     author  \\\n0      0                                         NC and NH.  Trumpbart   \n1      0  You do know west teams play against west teams...  Shbshb906   \n2      0  They were underdogs earlier today, but since G...   Creepeth   \n3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n4      0                    I could use one of those tools.  cush2push   \n\n            subreddit  score  ups  downs     date          created_utc  \\\n0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n\n                                      parent_comment  \n0  Yeah, I get that argument. At this point, I'd ...  \n1  The blazers and Mavericks (The wests 5 and 6 s...  \n2                            They're favored to win.  \n3                         deadass don't kill my buzz  \n4  Yep can confirm I saw the tool they use for th...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>comment</th>\n      <th>author</th>\n      <th>subreddit</th>\n      <th>score</th>\n      <th>ups</th>\n      <th>downs</th>\n      <th>date</th>\n      <th>created_utc</th>\n      <th>parent_comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NC and NH.</td>\n      <td>Trumpbart</td>\n      <td>politics</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-10</td>\n      <td>2016-10-16 23:55:23</td>\n      <td>Yeah, I get that argument. At this point, I'd ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>You do know west teams play against west teams...</td>\n      <td>Shbshb906</td>\n      <td>nba</td>\n      <td>-4</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-11</td>\n      <td>2016-11-01 00:24:10</td>\n      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>They were underdogs earlier today, but since G...</td>\n      <td>Creepeth</td>\n      <td>nfl</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2016-09</td>\n      <td>2016-09-22 21:45:37</td>\n      <td>They're favored to win.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>This meme isn't funny none of the \"new york ni...</td>\n      <td>icebrotha</td>\n      <td>BlackPeopleTwitter</td>\n      <td>-8</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-10</td>\n      <td>2016-10-18 21:03:47</td>\n      <td>deadass don't kill my buzz</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>I could use one of those tools.</td>\n      <td>cush2push</td>\n      <td>MaddenUltimateTeam</td>\n      <td>6</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-12</td>\n      <td>2016-12-30 17:00:13</td>\n      <td>Yep can confirm I saw the tool they use for th...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv('train-balanced-sarcasm.csv')\n",
    "raw_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T20:19:42.212716200Z",
     "start_time": "2024-03-05T20:19:39.385005200Z"
    }
   },
   "id": "24dae189076df892"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data-preprocessing "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c497a3e202b0d2f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drop Features\n",
    "\n",
    "Irrelevant: author, date, created_utc"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48d4d0e986e3736d"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    505405\n",
      "1    505368\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "   label                                            comment     author  \\\n0      0                                         NC and NH.  Trumpbart   \n1      0  You do know west teams play against west teams...  Shbshb906   \n2      0  They were underdogs earlier today, but since G...   Creepeth   \n3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n4      0                    I could use one of those tools.  cush2push   \n\n            subreddit  score  ups  downs     date          created_utc  \\\n0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n\n                                      parent_comment  \n0  Yeah, I get that argument. At this point, I'd ...  \n1  The blazers and Mavericks (The wests 5 and 6 s...  \n2                            They're favored to win.  \n3                         deadass don't kill my buzz  \n4  Yep can confirm I saw the tool they use for th...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>comment</th>\n      <th>author</th>\n      <th>subreddit</th>\n      <th>score</th>\n      <th>ups</th>\n      <th>downs</th>\n      <th>date</th>\n      <th>created_utc</th>\n      <th>parent_comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NC and NH.</td>\n      <td>Trumpbart</td>\n      <td>politics</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-10</td>\n      <td>2016-10-16 23:55:23</td>\n      <td>Yeah, I get that argument. At this point, I'd ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>You do know west teams play against west teams...</td>\n      <td>Shbshb906</td>\n      <td>nba</td>\n      <td>-4</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-11</td>\n      <td>2016-11-01 00:24:10</td>\n      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>They were underdogs earlier today, but since G...</td>\n      <td>Creepeth</td>\n      <td>nfl</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2016-09</td>\n      <td>2016-09-22 21:45:37</td>\n      <td>They're favored to win.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>This meme isn't funny none of the \"new york ni...</td>\n      <td>icebrotha</td>\n      <td>BlackPeopleTwitter</td>\n      <td>-8</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-10</td>\n      <td>2016-10-18 21:03:47</td>\n      <td>deadass don't kill my buzz</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>I could use one of those tools.</td>\n      <td>cush2push</td>\n      <td>MaddenUltimateTeam</td>\n      <td>6</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-12</td>\n      <td>2016-12-30 17:00:13</td>\n      <td>Yep can confirm I saw the tool they use for th...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop NAs\n",
    "raw_df.dropna(inplace=True)\n",
    "\n",
    "# Select 100000 rows of sample\n",
    "# Reset index so the cross validation later won't go wrong\n",
    "#filter_df = raw_df.sample(n=100000, random_state=000).reset_index(drop=True)\n",
    "filter_df = raw_df\n",
    "\n",
    "# Drop irrelevant features\n",
    "filter_df.drop(['author', 'date', 'created_utc'],axis=1)\n",
    "\n",
    "# Data is balance, do not need oversampling\n",
    "print(filter_df['label'].value_counts()) \n",
    "\n",
    "# Show the data\n",
    "filter_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T20:20:55.204097800Z",
     "start_time": "2024-03-05T20:20:54.369769200Z"
    }
   },
   "id": "625c11bd83cbd719"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Categorical Process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84b5a2a0de21ccb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform comments into TF-IDF vectors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c99f74fa3801eed"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1010773, 2450010)\n",
      "(1010773, 4926771)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the processed comments and parent_comment\n",
    "tfidf_comment = tfidf_vectorizer.fit_transform(filter_df['comment'])\n",
    "tfidf_parent_comment = tfidf_vectorizer.fit_transform(filter_df['parent_comment'])\n",
    "\n",
    "# Display the shape of the resulting TF-IDF feature matrix\n",
    "print(tfidf_comment.shape)\n",
    "print(tfidf_parent_comment.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T20:22:08.110996300Z",
     "start_time": "2024-03-05T20:20:58.613646800Z"
    }
   },
   "id": "2ed96c8eed9e6054"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have got a comment TF-IDF matrix, containing 100000 rows and 436490 features\n",
    "We have got a parent_comment TF-IDF matrix, containing 100000 rows and 899057 features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "466e80682cedbfaa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform categorical \"subreddit\" to dummy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ba0c5e80b8f7858"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(100000, 5594)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = ['subreddit']\n",
    "for i in categorical_columns: \n",
    "    filter_df = pd.concat([filter_df,pd.get_dummies(filter_df[i],drop_first=True, prefix=i)],axis=1)\n",
    "    filter_df = filter_df.drop(i,axis=1)\n",
    "    \n",
    "filter_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:09:34.898397800Z",
     "start_time": "2024-03-05T16:09:33.264259800Z"
    }
   },
   "id": "d61581906660faed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bb084cd41406207"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Y is the response variable\n",
    "Y = filter_df['label']\n",
    "\n",
    "# X is the features\n",
    "X = tfidf_comment\n",
    "\n",
    "# Split the data (Train 0.8, Test 0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T20:22:13.925871600Z",
     "start_time": "2024-03-05T20:22:13.768518700Z"
    }
   },
   "id": "4de6ae5139cb55a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-Fold CV Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f80331cb0ea8defe"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Set up K-Fold Cross Validation \n",
    "n_splits = 5\n",
    "shuffle = True\n",
    "random_state = 000\n",
    "cv = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T20:22:15.242328200Z",
     "start_time": "2024-03-05T20:22:15.226660300Z"
    }
   },
   "id": "9d7c518ab970ed35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Function to Create Dictionary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c08174d5304e7ea"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def create_dictionary(param_1,param_2):\n",
    "    result_dictionary = {} \n",
    "    for i in param_1: \n",
    "        result_dictionary[i] = {} \n",
    "        for j in param_2: \n",
    "                result_dictionary[i][j] = {} \n",
    "    return result_dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T20:22:17.035686900Z",
     "start_time": "2024-03-05T20:22:17.017540500Z"
    }
   },
   "id": "b6d10566970df04e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45ea6202bf1b29ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Random Forest Cross Validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3198496f12dfb10"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth of Tree :  2  Number of Trees  50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 27\u001B[0m\n\u001B[0;32m     24\u001B[0m X_cv_train, X_cv_test \u001B[38;5;241m=\u001B[39m X[train_index], X[test_index]\n\u001B[0;32m     25\u001B[0m Y_cv_train, Y_cv_test \u001B[38;5;241m=\u001B[39m Y[train_index], Y[test_index]\n\u001B[1;32m---> 27\u001B[0m random_forest_cv\u001B[38;5;241m.\u001B[39mfit(X_cv_train, Y_cv_train)\n\u001B[0;32m     28\u001B[0m Y_pred \u001B[38;5;241m=\u001B[39m random_forest_cv\u001B[38;5;241m.\u001B[39mpredict(X_cv_test)\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# Cross-Validation Prediction Error\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    462\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    463\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m    464\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[0;32m    465\u001B[0m ]\n\u001B[0;32m    467\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[0;32m    468\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[0;32m    469\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[0;32m    470\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[0;32m    471\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[0;32m    472\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[1;32m--> 473\u001B[0m trees \u001B[38;5;241m=\u001B[39m Parallel(\n\u001B[0;32m    474\u001B[0m     n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs,\n\u001B[0;32m    475\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[0;32m    476\u001B[0m     prefer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthreads\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    477\u001B[0m )(\n\u001B[0;32m    478\u001B[0m     delayed(_parallel_build_trees)(\n\u001B[0;32m    479\u001B[0m         t,\n\u001B[0;32m    480\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbootstrap,\n\u001B[0;32m    481\u001B[0m         X,\n\u001B[0;32m    482\u001B[0m         y,\n\u001B[0;32m    483\u001B[0m         sample_weight,\n\u001B[0;32m    484\u001B[0m         i,\n\u001B[0;32m    485\u001B[0m         \u001B[38;5;28mlen\u001B[39m(trees),\n\u001B[0;32m    486\u001B[0m         verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[0;32m    487\u001B[0m         class_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_weight,\n\u001B[0;32m    488\u001B[0m         n_samples_bootstrap\u001B[38;5;241m=\u001B[39mn_samples_bootstrap,\n\u001B[0;32m    489\u001B[0m     )\n\u001B[0;32m    490\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(trees)\n\u001B[0;32m    491\u001B[0m )\n\u001B[0;32m    493\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[0;32m    494\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     58\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     59\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     60\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     62\u001B[0m )\n\u001B[1;32m---> 63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1085\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[0;32m   1086\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1088\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[0;32m   1089\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1092\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[0;32m   1093\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[0;32m   1094\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    899\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    900\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 901\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dispatch(tasks)\n\u001B[0;32m    902\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    817\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    818\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 819\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mapply_async(batch, callback\u001B[38;5;241m=\u001B[39mcb)\n\u001B[0;32m    820\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    821\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    822\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    823\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    824\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m ImmediateResult(func)\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    594\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    595\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    596\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 597\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m batch()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    121\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001B[0m\n\u001B[0;32m    181\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m class_weight \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced_subsample\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    182\u001B[0m         curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m, y, indices\u001B[38;5;241m=\u001B[39mindices)\n\u001B[1;32m--> 184\u001B[0m     tree\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39mcurr_sample_weight, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    186\u001B[0m     tree\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39msample_weight, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001B[0m, in \u001B[0;36mDecisionTreeClassifier.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m    859\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    860\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001B[39;00m\n\u001B[0;32m    861\u001B[0m \n\u001B[0;32m    862\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    886\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m    887\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 889\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mfit(\n\u001B[0;32m    890\u001B[0m         X,\n\u001B[0;32m    891\u001B[0m         y,\n\u001B[0;32m    892\u001B[0m         sample_weight\u001B[38;5;241m=\u001B[39msample_weight,\n\u001B[0;32m    893\u001B[0m         check_input\u001B[38;5;241m=\u001B[39mcheck_input,\n\u001B[0;32m    894\u001B[0m     )\n\u001B[0;32m    895\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001B[0m, in \u001B[0;36mBaseDecisionTree.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m    368\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    369\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[0;32m    370\u001B[0m         splitter,\n\u001B[0;32m    371\u001B[0m         min_samples_split,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    376\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[0;32m    377\u001B[0m     )\n\u001B[1;32m--> 379\u001B[0m builder\u001B[38;5;241m.\u001B[39mbuild(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtree_, X, y, sample_weight)\n\u001B[0;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    382\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# CV in Trees\n",
    "# Set Hyperparameter (Lambda) values to cross validate\n",
    "max_depth = [2, 5, 10, 15, 20, 25]\n",
    "number_of_trees = [50, 100, 150, 200]\n",
    "\n",
    "cross_validate_result = create_dictionary(number_of_trees, max_depth)\n",
    "cross_validate_recall = create_dictionary(number_of_trees, max_depth)\n",
    "cross_validate_precision = create_dictionary(number_of_trees, max_depth)\n",
    "cross_validate_mse = create_dictionary(number_of_trees, max_depth)\n",
    "\n",
    "for tree in number_of_trees:\n",
    "    for depth in max_depth:\n",
    "        print('Depth of Tree : ', depth, ' Number of Trees ', tree)\n",
    "\n",
    "        accuracies = []\n",
    "        recall_scores = []\n",
    "        precision_scores = []\n",
    "        mse_scores = []\n",
    "\n",
    "        random_forest_cv = RandomForestClassifier(n_estimators=tree, max_depth=depth)\n",
    "\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            # define the rows in the dataframe\n",
    "            X_cv_train, X_cv_test = X[train_index], X[test_index]\n",
    "            Y_cv_train, Y_cv_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            random_forest_cv.fit(X_cv_train, Y_cv_train)\n",
    "            Y_pred = random_forest_cv.predict(X_cv_test)\n",
    "\n",
    "            # Cross-Validation Prediction Error\n",
    "            score = random_forest_cv.score(X_cv_test, Y_cv_test)\n",
    "            accuracies.append(score)\n",
    "            recall_scores.append(recall_score(Y_cv_test, Y_pred))\n",
    "            precision_scores.append(precision_score(Y_cv_test, Y_pred))\n",
    "            mse_scores.append(mean_squared_error(Y_cv_test, Y_pred))\n",
    "\n",
    "        cross_validate_result[tree][depth] = (sum(accuracies) / len(accuracies))\n",
    "        cross_validate_recall[tree][depth] = (sum(recall_scores) / len(recall_scores))\n",
    "        cross_validate_precision[tree][depth] = (sum(precision_scores) / len(precision_scores))\n",
    "        cross_validate_mse[tree][depth] = (sum(mse_scores) / len(mse_scores))\n",
    "\n",
    "        print(\"Accuracy : \" + str((sum(accuracies) / len(accuracies))))\n",
    "        print(\"Precision : \" + str((sum(recall_scores) / len(recall_scores))))\n",
    "        print(\"Recall : \" + str((sum(precision_scores) / len(precision_scores))))\n",
    "        print(\"MSE : \" + str((sum(mse_scores) / len(mse_scores))))\n",
    "        print()\n",
    "\n",
    "# Dictionary Summary\n",
    "print('------------------')\n",
    "print('Accuracy : ', cross_validate_result)\n",
    "print('Precision : ', cross_validate_precision)\n",
    "print('Recall : ', cross_validate_recall)\n",
    "print('MSE : ', cross_validate_mse)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:19:06.075708200Z",
     "start_time": "2024-03-04T14:18:51.243926800Z"
    }
   },
   "id": "ae676bf8916158e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91e3191274b13ebf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Define line styles and colors for different number of trees\n",
    "line_styles = {50: 'solid', 100: 'dashed', 150: 'dotted', 200: 'dashdot'}\n",
    "colors = {'Accuracy': 'blue', 'Precision': 'green', 'Recall': 'red', 'MSE': 'grey'}\n",
    "\n",
    "# Consolidate plotting data\n",
    "metrics = {\n",
    "    'Accuracy': cross_validate_result,\n",
    "    'Precision': cross_validate_precision,\n",
    "    'Recall': cross_validate_recall,\n",
    "    'MSE': cross_validate_mse,\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "for metric_name, metric_data in metrics.items():\n",
    "    for num_trees, depths in metric_data.items():\n",
    "        plt.plot(max_depth, [depths[depth] for depth in max_depth],\n",
    "                 label=f'{metric_name} ({num_trees} trees)',\n",
    "                 linestyle=line_styles[num_trees],\n",
    "                 color=colors[metric_name])\n",
    "\n",
    "plt.title('Model Performance by Max Depth and Number of Trees')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:19:06.076694Z",
     "start_time": "2024-03-04T14:19:06.076694Z"
    }
   },
   "id": "a906cd9ccb12b32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After selected the best parameter, run the random forest OOS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d9e13b4a8282a84"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RND Forest Accuracy :  0.6675916994385497\n",
      "RND Forest Recall :  0.5343611985819816\n",
      "RND Forest Precision :  0.7278723461652594\n",
      "RND Forest F1 :  0.6162833192480757\n",
      "RND Forest MSE :  0.3324083005614504\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators = 200, max_depth=10, random_state=0)\n",
    "random_forest.fit(X, Y)\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "print(\"RND Forest Accuracy : \" , accuracy_score(y_test, y_pred))\n",
    "print(\"RND Forest Recall : \" , recall_score(y_test, y_pred))\n",
    "print(\"RND Forest Precision : \", precision_score(y_test,y_pred))\n",
    "print(\"RND Forest F1 : \", f1_score(y_test,y_pred))\n",
    "print(\"RND Forest MSE : \", mean_squared_error(y_test,y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T20:24:12.903377600Z",
     "start_time": "2024-03-05T20:22:24.073064Z"
    }
   },
   "id": "ae634c8cbb2dd87b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract the random forest coefficient to see its importance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8c2576652c2a147"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "later snowboard: 0.017216191437231947\n",
      "cameras conveniently: 0.01491188864063314\n",
      "hillary would: 0.011223054360433207\n",
      "just freezes: 0.009869132529134393\n",
      "camille deserves: 0.009790887421415524\n",
      "judge what: 0.009211462689342129\n",
      "happens won: 0.00898179141521509\n",
      "amazeballs why: 0.008469001460810463\n",
      "fucking giraffe: 0.008163975028692021\n",
      "inspectors ad: 0.008129785720232178\n",
      "fast method: 0.007830875326338296\n",
      "hebrews just: 0.007592631274140501\n",
      "565 rest: 0.006958159439414971\n",
      "american dnr: 0.006457834724192914\n",
      "city reputation: 0.0064499339152156955\n",
      "around what: 0.006386466207715914\n",
      "land damaging: 0.00533912693940992\n",
      "kershaw shocking: 0.005027458234483149\n",
      "fame each: 0.004960030623059647\n",
      "gets squeed: 0.004859906694155918\n",
      "later issued: 0.004799917116109559\n",
      "at dncmedaparty: 0.004727503193279378\n",
      "foundation tax: 0.004266122935963653\n",
      "apart via: 0.00426259473309641\n",
      "impossible super: 0.004138159023838988\n",
      "exactly sex: 0.004124119785610934\n",
      "first motorcycle: 0.0040412519522353765\n",
      "later waiting: 0.004011488075315575\n",
      "greens efa: 0.0038839735882683296\n",
      "feeling tired: 0.0038195636405100337\n",
      "adding all: 0.003771423294945435\n",
      "lallana add: 0.003739896102145109\n",
      "drams is: 0.0037163282018366876\n",
      "another tech: 0.003617749735427183\n",
      "launch gnome: 0.0035053993171859283\n",
      "amendment introduced: 0.003491476638494683\n",
      "lather up: 0.00333719602779285\n",
      "immortals roster: 0.0033101283780165164\n",
      "american activities: 0.003309791892045037\n",
      "and irresponsible: 0.003295682331390417\n",
      "for qb: 0.0030502620899811096\n",
      "children dead: 0.003047110856208291\n",
      "fucking fast: 0.0030056937041863757\n",
      "cymbal we: 0.0029783836717654734\n",
      "camille lately: 0.0029779737207747386\n",
      "agile master: 0.002975370014815467\n",
      "bho: 0.0029702295754318914\n",
      "face probably: 0.002905353656281905\n",
      "league bud: 0.002881803387947961\n",
      "extreme discounts: 0.0028785447812183424\n"
     ]
    }
   ],
   "source": [
    "idf_values = tfidf_vectorizer.idf_\n",
    "\n",
    "# Get Feature Importance\n",
    "importance = random_forest.feature_importances_\n",
    "\n",
    "# Get Feature Name\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Pair Feature Importance and Name\n",
    "features_importance = zip(feature_names, importance)\n",
    "\n",
    "# Pair Feature IDF Value and Name\n",
    "features_idf = zip(feature_names, idf_values)\n",
    "\n",
    "# Sort feature by importance\n",
    "sorted_features = sorted(features_importance, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get first 50\n",
    "for feature, importance in sorted_features[:50]:\n",
    "    print(f\"{feature}: {importance}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T20:24:28.413707500Z",
     "start_time": "2024-03-05T20:24:21.440145300Z"
    }
   },
   "id": "f06ac67763bc9d87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eliminate Stop Words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b18006bc4b4b68a5"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\16920\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "later snowboard: 0.017216191437231947\n",
      "cameras conveniently: 0.01491188864063314\n",
      "hillary would: 0.011223054360433207\n",
      "just freezes: 0.009869132529134393\n",
      "camille deserves: 0.009790887421415524\n",
      "judge what: 0.009211462689342129\n",
      "happens won: 0.00898179141521509\n",
      "amazeballs why: 0.008469001460810463\n",
      "fucking giraffe: 0.008163975028692021\n",
      "inspectors ad: 0.008129785720232178\n",
      "fast method: 0.007830875326338296\n",
      "hebrews just: 0.007592631274140501\n",
      "565 rest: 0.006958159439414971\n",
      "american dnr: 0.006457834724192914\n",
      "city reputation: 0.0064499339152156955\n",
      "around what: 0.006386466207715914\n",
      "land damaging: 0.00533912693940992\n",
      "kershaw shocking: 0.005027458234483149\n",
      "fame each: 0.004960030623059647\n",
      "gets squeed: 0.004859906694155918\n",
      "later issued: 0.004799917116109559\n",
      "at dncmedaparty: 0.004727503193279378\n",
      "foundation tax: 0.004266122935963653\n",
      "apart via: 0.00426259473309641\n",
      "impossible super: 0.004138159023838988\n",
      "exactly sex: 0.004124119785610934\n",
      "first motorcycle: 0.0040412519522353765\n",
      "later waiting: 0.004011488075315575\n",
      "greens efa: 0.0038839735882683296\n",
      "feeling tired: 0.0038195636405100337\n",
      "adding all: 0.003771423294945435\n",
      "lallana add: 0.003739896102145109\n",
      "drams is: 0.0037163282018366876\n",
      "another tech: 0.003617749735427183\n",
      "launch gnome: 0.0035053993171859283\n",
      "amendment introduced: 0.003491476638494683\n",
      "lather up: 0.00333719602779285\n",
      "immortals roster: 0.0033101283780165164\n",
      "american activities: 0.003309791892045037\n",
      "and irresponsible: 0.003295682331390417\n",
      "for qb: 0.0030502620899811096\n",
      "children dead: 0.003047110856208291\n",
      "fucking fast: 0.0030056937041863757\n",
      "cymbal we: 0.0029783836717654734\n",
      "camille lately: 0.0029779737207747386\n",
      "agile master: 0.002975370014815467\n",
      "bho: 0.0029702295754318914\n",
      "face probably: 0.002905353656281905\n",
      "league bud: 0.002881803387947961\n",
      "extreme discounts: 0.0028785447812183424\n",
      "entirely simple: 0.0028686692610475446\n",
      "installed one: 0.002859544826723286\n",
      "being once: 0.0027994492969708757\n",
      "him fart: 0.0027710954254811\n",
      "apparent impunity: 0.002672158669828577\n",
      "elway to: 0.0026612211109922177\n",
      "basically tanked: 0.0026546757026549984\n",
      "labor forces: 0.0026500391240544016\n",
      "elysiandreams: 0.0026312783728580326\n",
      "lahouaiej bouhlel: 0.00260739183634253\n",
      "and gutter: 0.0026063938520744625\n",
      "cultures changes: 0.002547934929581145\n",
      "hp vero: 0.002535748560668826\n",
      "brasil germany: 0.0024823360428148965\n",
      "buu cell: 0.002427491182129728\n",
      "camille grammer: 0.0024073940848476447\n",
      "5th including: 0.0023011009622054394\n",
      "it smashed: 0.002287345432600767\n",
      "better bandwagon: 0.002239910825635685\n",
      "about stolen: 0.002227645043145277\n",
      "have batcave: 0.002172794824334537\n",
      "least slutty: 0.002165624001355679\n",
      "hes putting: 0.002159134735055462\n",
      "lawyers guy: 0.002128097621834879\n",
      "60gb nevermind: 0.0021237485005175774\n",
      "every evasion: 0.00206892603128239\n",
      "independence party: 0.002066463010942062\n",
      "disaster relief: 0.002061373739163462\n",
      "impunity the: 0.002043130410967899\n",
      "in gaithersburg: 0.002035832911697415\n",
      "isn included: 0.0020203261392438927\n",
      "dead seregios: 0.0020145192582708494\n",
      "kougar klub: 0.002002185856515621\n",
      "gump game: 0.0019983422289611597\n",
      "chaos they: 0.0019948698915997027\n",
      "launch mobile: 0.0019447699225232165\n",
      "has comedy: 0.0019413952813781157\n",
      "72 benchmarks: 0.0019393991269630983\n",
      "hellbent maniac: 0.0019320507180718496\n",
      "keer: 0.001924241714605733\n",
      "be impartial: 0.0019112407200592193\n",
      "don recommended: 0.0018404595003105985\n",
      "could vary: 0.0018376701113055952\n",
      "imperfect world: 0.0018362073099309512\n",
      "fussier character: 0.0018227697278354116\n",
      "caveat can: 0.0018188029122515334\n",
      "kevin ollie: 0.0018183018396335285\n",
      "launched my: 0.001813301391475901\n",
      "horton blah: 0.0018132384756181373\n",
      "enough internet: 0.0018078564552056057\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter Stop Words\n",
    "filtered_features = [(feature, importance) for feature, importance in sorted_features if feature not in stop_words]\n",
    "\n",
    "# Print Filtered Features\n",
    "for feature, importance in filtered_features[:100]: # Get First 100\n",
    "    print(f\"{feature}: {importance}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T20:26:59.006856100Z",
     "start_time": "2024-03-05T20:26:58.286292100Z"
    }
   },
   "id": "c9d30be5beb4129"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bd0609a3e7a8ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 分别提取特征名和重要性得分\n",
    "features = [feature for feature, _ in filtered_features]\n",
    "importances = [importance for _, importance in filtered_features]\n",
    "\n",
    "# 绘图\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(features, importances, color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()  # 反转y轴，使最重要的特征显示在顶部\n",
    "plt.show()\n",
    "\n",
    "#待跑"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-05T20:27:01.103282900Z"
    }
   },
   "id": "c1e65e201e6236a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=importances, y=features, palette=\"viridis\")\n",
    "\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Top Features Importance from Random Forest (Filtered)')\n",
    "plt.show()\n",
    "\n",
    "#待跑"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-04T14:26:33.391365500Z"
    }
   },
   "id": "ef27a8ab19ec90bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adaboost"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d90305e2a931fd29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adaptive Boosting Algorithm Cross Validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9e493ab854ab044"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth of Tree :  50  Learning rate  0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 28\u001B[0m\n\u001B[0;32m     25\u001B[0m X_cv_train, X_cv_test \u001B[38;5;241m=\u001B[39m X[train_index], X[test_index]\n\u001B[0;32m     26\u001B[0m Y_cv_train, Y_cv_test \u001B[38;5;241m=\u001B[39m Y[train_index], Y[test_index]\n\u001B[1;32m---> 28\u001B[0m adaboost_classifier\u001B[38;5;241m.\u001B[39mfit(X_cv_train, Y_cv_train)\n\u001B[0;32m     29\u001B[0m Y_pred \u001B[38;5;241m=\u001B[39m adaboost_classifier\u001B[38;5;241m.\u001B[39mpredict(X_cv_test)\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m#Cross-Validation Prediction Error\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:162\u001B[0m, in \u001B[0;36mBaseWeightBoosting.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    159\u001B[0m sample_weight[zero_weight_mask] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;66;03m# Boosting step\u001B[39;00m\n\u001B[1;32m--> 162\u001B[0m sample_weight, estimator_weight, estimator_error \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_boost(\n\u001B[0;32m    163\u001B[0m     iboost, X, y, sample_weight, random_state\n\u001B[0;32m    164\u001B[0m )\n\u001B[0;32m    166\u001B[0m \u001B[38;5;66;03m# Early termination\u001B[39;00m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:569\u001B[0m, in \u001B[0;36mAdaBoostClassifier._boost\u001B[1;34m(self, iboost, X, y, sample_weight, random_state)\u001B[0m\n\u001B[0;32m    530\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Implement a single boost.\u001B[39;00m\n\u001B[0;32m    531\u001B[0m \n\u001B[0;32m    532\u001B[0m \u001B[38;5;124;03mPerform a single boost according to the real multi-class SAMME.R\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    566\u001B[0m \u001B[38;5;124;03m    If None then boosting has terminated early.\u001B[39;00m\n\u001B[0;32m    567\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malgorithm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSAMME.R\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 569\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_boost_real(iboost, X, y, sample_weight, random_state)\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# elif self.algorithm == \"SAMME\":\u001B[39;00m\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_boost_discrete(iboost, X, y, sample_weight, random_state)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:578\u001B[0m, in \u001B[0;36mAdaBoostClassifier._boost_real\u001B[1;34m(self, iboost, X, y, sample_weight, random_state)\u001B[0m\n\u001B[0;32m    575\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\u001B[39;00m\n\u001B[0;32m    576\u001B[0m estimator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[1;32m--> 578\u001B[0m estimator\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39msample_weight)\n\u001B[0;32m    580\u001B[0m y_predict_proba \u001B[38;5;241m=\u001B[39m estimator\u001B[38;5;241m.\u001B[39mpredict_proba(X)\n\u001B[0;32m    582\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m iboost \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001B[0m, in \u001B[0;36mDecisionTreeClassifier.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m    859\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    860\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001B[39;00m\n\u001B[0;32m    861\u001B[0m \n\u001B[0;32m    862\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    886\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m    887\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 889\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mfit(\n\u001B[0;32m    890\u001B[0m         X,\n\u001B[0;32m    891\u001B[0m         y,\n\u001B[0;32m    892\u001B[0m         sample_weight\u001B[38;5;241m=\u001B[39msample_weight,\n\u001B[0;32m    893\u001B[0m         check_input\u001B[38;5;241m=\u001B[39mcheck_input,\n\u001B[0;32m    894\u001B[0m     )\n\u001B[0;32m    895\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001B[0m, in \u001B[0;36mBaseDecisionTree.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m    368\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    369\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[0;32m    370\u001B[0m         splitter,\n\u001B[0;32m    371\u001B[0m         min_samples_split,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    376\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[0;32m    377\u001B[0m     )\n\u001B[1;32m--> 379\u001B[0m builder\u001B[38;5;241m.\u001B[39mbuild(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtree_, X, y, sample_weight)\n\u001B[0;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    382\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# CV in Trees \n",
    "# Set Hyperparameter (Lambda) values to cross validate\n",
    "\n",
    "# Create a weak learner (a stump)\n",
    "weak_learner = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "number_of_trees_ada = [50, 100, 150, 200]\n",
    "\n",
    "cross_validate_result = create_dictionary(learning_rates,number_of_trees_ada)\n",
    "cross_validate_recall = create_dictionary(learning_rates,number_of_trees_ada)\n",
    "cross_validate_precision = create_dictionary(learning_rates,number_of_trees_ada) \n",
    "cross_validate_mse = create_dictionary(learning_rates,number_of_trees_ada)\n",
    "\n",
    "for rate in learning_rates: \n",
    "    for tree in number_of_trees_ada:\n",
    "        print('Depth of Tree : ', tree, ' Learning rate ', rate)\n",
    "        accuracies = []\n",
    "        recall_scores = [] \n",
    "        precision_scores = [] \n",
    "        mse_scores = []\n",
    "        adaboost_classifier = AdaBoostClassifier(estimator=weak_learner, n_estimators=tree, random_state=0)\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            # define the rows in the dataframe\n",
    "            X_cv_train, X_cv_test = X[train_index], X[test_index]\n",
    "            Y_cv_train, Y_cv_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            adaboost_classifier.fit(X_cv_train, Y_cv_train)\n",
    "            Y_pred = adaboost_classifier.predict(X_cv_test)\n",
    "\n",
    "            #Cross-Validation Prediction Error\n",
    "            score = adaboost_classifier.score(X_cv_test, Y_cv_test)\n",
    "            accuracies.append(score)\n",
    "            recall_scores.append(recall_score(Y_cv_test, Y_pred))\n",
    "            precision_scores.append(precision_score(Y_cv_test,Y_pred))\n",
    "            mse_scores.append(mean_squared_error(Y_cv_test, Y_pred))\n",
    "        \n",
    "        cross_validate_result[rate][tree] = (sum(accuracies)/len(accuracies))\n",
    "        cross_validate_recall[rate][tree] = (sum(recall_scores)/len(recall_scores))\n",
    "        cross_validate_precision[rate][tree] = (sum(precision_scores)/len(precision_scores))\n",
    "        cross_validate_mse[rate][tree] = (sum(mse_scores)/len(mse_scores))\n",
    "\n",
    "        print(\"Accuracy : \" + str((sum(accuracies)/len(accuracies))))\n",
    "        print(\"Precision : \" + str((sum(recall_scores)/len(recall_scores))))\n",
    "        print(\"Recall : \" + str((sum(precision_scores)/len(precision_scores))))\n",
    "        print(\"MSE : \" + str((sum(mse_scores)/len(mse_scores))))\n",
    "        print() \n",
    "\n",
    "# Dictionary Summary\n",
    "print('------------------')\n",
    "print('Accuracy : ', cross_validate_result)\n",
    "print('Precision : ', cross_validate_precision)\n",
    "print('Recall : ', cross_validate_recall)\n",
    "print('MSE : ', cross_validate_mse)\n",
    "\n",
    "#待跑"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T06:35:16.590052200Z",
     "start_time": "2024-03-05T04:42:31.869643800Z"
    }
   },
   "id": "fe2424d1dc8a2f5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc6f30fe6c93ef0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Define line styles and colors for different number of trees\n",
    "line_styles = {50: 'solid', 100: 'dashed', 150: 'dotted', 200: 'dashdot'}\n",
    "colors = {'Accuracy': 'blue', 'Precision': 'green', 'Recall': 'red', 'MSE': 'grey'}\n",
    "\n",
    "# Consolidate plotting data\n",
    "metrics = {\n",
    "    'Accuracy': cross_validate_result,\n",
    "    'Precision': cross_validate_precision,\n",
    "    'Recall': cross_validate_recall,\n",
    "    'MSE': cross_validate_mse,\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "for metric_name, metric_data in metrics.items():\n",
    "    for num_trees, rates in metric_data.items():\n",
    "        plt.plot(number_of_trees_ada, [rates[rate] for rate in rates],\n",
    "                 label=f'{metric_name} ({num_trees} trees)',\n",
    "                 linestyle=line_styles[num_trees],\n",
    "                 color=colors[metric_name])\n",
    "\n",
    "plt.title('Model Performance by Learning Rate and Number of Trees')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "629267a2b7400ca8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adaboost Rerun"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b02b42a9a0bfc00d"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_smote' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m adaboost_classifier \u001B[38;5;241m=\u001B[39m AdaBoostClassifier(estimator\u001B[38;5;241m=\u001B[39mweak_learner, n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Train the AdaBoost classifier\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m adaboost_classifier\u001B[38;5;241m.\u001B[39mfit(X_smote, y_smote)\n\u001B[0;32m     10\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m adaboost_classifier\u001B[38;5;241m.\u001B[39mpredict(X_OOS_test)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAdaboost Accuracy : \u001B[39m\u001B[38;5;124m\"\u001B[39m , accuracy_score(y_OOS_test, y_pred))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X_smote' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a weak learner (a stump)\n",
    "weak_learner = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Create an AdaBoost classifier using the weak learner\n",
    "adaboost_classifier = AdaBoostClassifier(estimator=weak_learner, n_estimators=1000, random_state=0)\n",
    "\n",
    "# Train the AdaBoost classifier\n",
    "adaboost_classifier.fit(X, Y)\n",
    "\n",
    "y_pred = adaboost_classifier.predict(X_test)\n",
    "print(\"Adaboost Accuracy : \" , accuracy_score(Y_test, Y_pred))\n",
    "print(\"Adaboost Recall : \" , recall_score(y_OOS_test, y_pred))\n",
    "print(\"Adaboost Precision : \", precision_score(y_OOS_test,y_pred))\n",
    "print(\"Adaboost f1 : \", f1_score(y_OOS_test,y_pred))\n",
    "print(\"Adaboost MSE : \", mean_squared_error(y_OOS_test,y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T06:36:05.414125400Z",
     "start_time": "2024-03-05T06:36:05.371823800Z"
    }
   },
   "id": "899856bea093899d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
